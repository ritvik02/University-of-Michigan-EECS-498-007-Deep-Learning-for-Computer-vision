{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"two_layer_net.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zNmLmqrJAXXp"},"source":["# EECS 498-007/598-005 Assignment 2-2: Two Layer Neural Network\n"]},{"cell_type":"markdown","metadata":{"id":"tUGCJrp9Aegm"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"eYE9thuXn4zP"},"source":["# Setup Code\n","Before getting started, we need to run some boilerplate code to set up our environment, same as Assignment 1. You'll need to rerun this setup code each time you start the notebook.\n","\n","First, run this cell load the autoreload extension. This allows us to edit .py source files, and re-import them into the notebook for a seamless editing and debugging experience."]},{"cell_type":"code","metadata":{"id":"QqEfH2Rpn9J3","executionInfo":{"status":"ok","timestamp":1622557227114,"user_tz":-330,"elapsed":2014,"user":{"displayName":"Ritvik Khandelwal","photoUrl":"","userId":"02924828484852851932"}}},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdowvtJen-IP"},"source":["## Google Colab Setup\n","Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n","\n","Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5yufsaboBqJ","executionInfo":{"status":"ok","timestamp":1622557451847,"user_tz":-330,"elapsed":224737,"user":{"displayName":"Ritvik Khandelwal","photoUrl":"","userId":"02924828484852851932"}},"outputId":"2b42e5a3-3dc4-4c64-9069-b29e635d760a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"32Fdd6a5oIT5"},"source":["Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n","\n","```\n","['two_layer_net.ipynb', 'eecs598', 'two_layer_net.py', 'linear_classifier.py', 'linear_classifier.ipynb', 'a2_helpers.py']\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsWH3c7VoL_A","executionInfo":{"status":"ok","timestamp":1622468386995,"user_tz":-330,"elapsed":413,"user":{"displayName":"Ritvik Khandelwal","photoUrl":"","userId":"02924828484852851932"}},"outputId":"44393a50-0cb8-40da-800e-4ec598ff7701"},"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the assignment\n","# Example: If you create a 2020FA folder and put all the files under A2 folder, then '2020FA/A2'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2020FA/A2'\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Umich EECS 498-007 DL4CV/A2'\n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['two_layer_net.ipynb', 'eecs598', '__pycache__', 'svm_best_model.pt', 'linear_classifier.py', 'softmax_best_model.pt', 'linear_classifier.ipynb', 'a2_helpers.py', 'two_layer_net.py']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GaCqHOm9oPB3"},"source":["Once you have successfully mounted your Google Drive and located the path to this assignment, run th following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n","\n","```\n","Hello from two_layer_net.py!\n","Hello from a2_helpers.py!\n","```\n","\n","as well as the last edit time for the file `two_layer_net.py`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCaNVx6JoWid","executionInfo":{"status":"ok","timestamp":1622468396380,"user_tz":-330,"elapsed":5068,"user":{"displayName":"Ritvik Khandelwal","photoUrl":"","userId":"02924828484852851932"}},"outputId":"54738253-13af-404b-cd4e-e89aa3352acd"},"source":["import sys\n","sys.path.append(GOOGLE_DRIVE_PATH)\n","\n","import time, os\n","os.environ[\"TZ\"] = \"US/Eastern\"\n","time.tzset()\n","\n","from two_layer_net import hello_two_layer_net\n","hello_two_layer_net()\n","\n","from a2_helpers import hello_helper\n","hello_helper()\n","\n","two_layer_net_path = os.path.join(GOOGLE_DRIVE_PATH, 'two_layer_net.py')\n","two_layer_net_edit_time = time.ctime(os.path.getmtime(two_layer_net_path))\n","print('two_layer_net.py last edited on %s' % two_layer_net_edit_time)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hello from two_layer_net.py!\n","Hello from a2_helpers.py!\n","two_layer_net.py last edited on Mon May 31 06:58:43 2021\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KtMy3qeipNK3"},"source":["## Miscellaneous"]},{"cell_type":"markdown","metadata":{"id":"z6WjZGY8A9CI"},"source":["Run some setup code for this notebook: Import some useful packages and increase the default figure size."]},{"cell_type":"code","metadata":{"id":"O3EvIZ0uAOVN"},"source":["import eecs598\n","import torch\n","import matplotlib.pyplot as plt\n","import statistics\n","import random\n","import time\n","%matplotlib inline\n","\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","plt.rcParams['font.size'] = 16\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OvUDZWGU3VLV"},"source":["We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrAX9FOLpr9k","executionInfo":{"status":"ok","timestamp":1622446039279,"user_tz":-330,"elapsed":1580,"user":{"displayName":"Ritvik Khandelwal","photoUrl":"","userId":"02924828484852851932"}},"outputId":"2e2af9c1-4794-4c0d-c641-be7c0cacd11e"},"source":["if torch.cuda.is_available:\n","  print('Good to go!')\n","else:\n","  print('Please set GPU via Edit -> Notebook Settings.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Good to go!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hbe3wUpVAjma"},"source":["# Implementing a Neural Network\n","In this exercise we will develop a neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n","\n","We train the network with a softmax loss function and L2 regularization on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. \n","\n","In other words, the network has the following architecture:\n","\n","  input - fully connected layer - ReLU - fully connected layer - softmax\n","\n","The outputs of the second fully-connected layer are the scores for each class.\n","\n","**Note**: When you implment the regularization over W, **please DO NOT multiply the regularization term by 1/2** (no coefficient). \n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lJqim3P1qZgv"},"source":["## Play with a toy data"]},{"cell_type":"markdown","metadata":{"id":"5T-4Phbd9GvI"},"source":["The inputs to our network will be a batch of $N$ (`num_inputs`) $D$-dimensional vectors (`input_size`); the hidden layer will have $H$ hidden units (`hidden_size`), and we will predict classification scores for $C$ categories (`num_classes`). This means that the learnable weights and biases of the network will have the following shapes:\n","\n","*   W1: First layer weights; has shape (D, H)\n","*   b1: First layer biases; has shape (H,)\n","*   W2: Second layer weights; has shape (H, C)\n","*   b2: Second layer biases; has shape (C,)\n","\n","We will use `a2_helpers.get_toy_data` function to generate random weights for a small toy model while we implement the model."]},{"cell_type":"markdown","metadata":{"id":"ZLdCF3B-AOVT"},"source":["### Forward pass: compute scores\n","Like in the Linear Classifiers exercise, we want to write a function that takes as input the model weights and a batch of images and labels, and returns the loss and the gradient of the loss with respect to each model parameter.\n","\n","However rather than attempting to implement the entire function at once, we will take a staged approach and ask you to implement the full forward and backward pass one step at a time.\n","\n","First we will implement the forward pass of the network which uses the weights and biases to compute scores for all inputs in `nn_forward_pass`."]},{"cell_type":"markdown","metadata":{"id":"inlH2l-XEtZQ"},"source":["Compute the scores and compare with the answer. The distance gap should be smaller than 1e-10."]},{"cell_type":"code","metadata":{"id":"1PQQdVj111Oj"},"source":["toy_X = 10.0 * torch.randn(N, D, device=device, dtype=dtype)\n","toy_y = torch.tensor([0, 1, 2, 2, 1], device=device, dtype=torch.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"id":"tZV9_3ZWAOVU","executionInfo":{"status":"error","timestamp":1622460245345,"user_tz":-330,"elapsed":1993,"user":{"displayName":"Ritvik Khandelwal","photoUrl":"","userId":"02924828484852851932"}},"outputId":"80db70d8-43ef-430b-8087-e96347930155"},"source":["import eecs598\n","from a2_helpers import get_toy_data\n","from two_layer_net import nn_forward_pass\n","\n","eecs598.reset_seed(0)\n","toy_X, toy_y, params = get_toy_data()\n","\n","# YOUR_TURN: Implement the score computation part of nn_forward_pass\n","scores, _ = nn_forward_pass(params, toy_X)\n","print('Your scores:')\n","print(scores)\n","print(scores.dtype)\n","print()\n","print('correct scores:')\n","correct_scores = torch.tensor([\n","        [ 9.7003e-08, -1.1143e-07, -3.9961e-08],\n","        [-7.4297e-08,  1.1502e-07,  1.5685e-07],\n","        [-2.5860e-07,  2.2765e-07,  3.2453e-07],\n","        [-4.7257e-07,  9.0935e-07,  4.0368e-07],\n","        [-1.8395e-07,  7.9303e-08,  6.0360e-07]], dtype=torch.float32, device=scores.device)\n","print(correct_scores)\n","print()\n","\n","# The difference should be very small. We get < 1e-10\n","scores_diff = (scores - correct_scores).abs().sum().item()\n","print('Difference between your scores and correct scores: %.2e' % scores_diff)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-9984787607b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meecs598\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtoy_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoy_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_toy_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# YOUR_TURN: Implement the score computation part of nn_forward_pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Umich EECS 498-007 DL4CV/A2/a2_helpers.py\u001b[0m in \u001b[0;36mget_toy_data\u001b[0;34m(num_inputs, input_size, hidden_size, num_classes, dtype, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;31m# Generate some random parameters, storing them in a dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"]}]},{"cell_type":"markdown","metadata":{"id":"7XNJ3ydEAOVW"},"source":["### Forward pass: compute loss\n","Now, we implement the first part of `nn_forward_backward` that computes the data and regularization loss.\n","\n","For the data loss, we will use the softmax loss. For the regularization loss we will use L2 regularization on the weight matrices `W1` and `W2`; we will not apply regularization loss to the bias vectors `b1` and `b2`."]},{"cell_type":"markdown","metadata":{"id":"C734SdJGE6xh"},"source":["First, Let's run the following to check your implementation.\n","\n","We compute the loss for the toy data, and compare with the answer computed by our implementation. The difference between the correct and computed loss should be less than `1e-4`."]},{"cell_type":"code","metadata":{"id":"3-arsG64_kIm"},"source":["# rough \n","eecs598.reset_seed(0)\n","toy_X, toy_y, params = get_toy_data()\n","W1, b1 = params['W1'], params['b1']\n","W2, b2 = params['W2'], params['b2']\n","N, D = X.shape\n","\n","#forward\n","hidden = X.mm(W1) + b1\n","pre_relu = hidden.clone()\n","hidden[hidden < 0] = 0\n","scores = hidden.mm(W2) + b2\n","\n","#forward_backward \n","scores -= torch.max(scores) # numeric stability\n","allexp = torch.exp(scores)\n","sumup = torch.sum(allexp, dim = 1)\n","correct = allexp[torch.arange(N), y]\n","prob = correct / sumup # only extract correct label\n","loss = -torch.sum(torch.log(prob))/N + reg * torch.sum(W1 * W1)  + reg * torch.sum(W2 * W2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wgG6w2uKAOVX"},"source":["import eecs598\n","from a2_helpers import get_toy_data\n","from two_layer_net import nn_forward_backward\n","\n","eecs598.reset_seed(0)\n","toy_X, toy_y, params = get_toy_data()\n","\n","# YOUR_TURN: Implement the loss computation part of nn_forward_backward\n","loss, _ = nn_forward_backward(params, toy_X, toy_y, reg=0.05)\n","print('Your loss: ', loss.item())\n","correct_loss = 1.0986121892929077\n","print('Correct loss: ', correct_loss)\n","diff = (correct_loss - loss).item()\n","\n","# should be very small, we get < 1e-4\n","print('Difference: %.4e' % diff)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vExP-7n3AOVa"},"source":["### Backward pass\n","Now implement the backward pass for the entire network in `nn_forward_backward`.\n","\n","After doing so, we will use numeric gradient checking to see whether the analytic gradient computed by our backward pass mateches a numeric gradient.\n","\n","We will use the functions `eecs598.grad.compute_numeric_gradient` and `eecs598.grad.rel_error` to help with numeric gradient checking. We can learn more about these functions using the `help` command:\n"]},{"cell_type":"code","metadata":{"id":"CJitZg6cS8Sf"},"source":["help(eecs598.grad.compute_numeric_gradient)\n","print('-' * 80)\n","help(eecs598.grad.rel_error)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93oOdibtW_Kl"},"source":["Now we will compute the gradient of the loss with respect to the variables `W1`, `b1`, `W2`, and `b2`. Now that you (hopefully!) have a correctly implemented forward pass, you can debug your backward pass using a numeric gradient check.\n","\n","You should see relative errors less than `1e-4` for all parameters."]},{"cell_type":"code","metadata":{"id":"qCEkprvoAOVb"},"source":["import eecs598\n","from a2_helpers import get_toy_data\n","from two_layer_net import nn_forward_backward\n","\n","eecs598.reset_seed(0)\n","\n","reg = 0.05\n","toy_X, toy_y, params = get_toy_data(dtype=torch.float64)\n","\n","# YOUR_TURN: Implement the gradient computation part of nn_forward_backward\n","#            When you implement the gradient computation part, you may need to \n","#            implement the `hidden` output in nn_forward_pass, as well.\n","loss, grads = nn_forward_backward(params, toy_X, toy_y, reg=reg)\n","\n","for param_name, grad in grads.items():\n","  param = params[param_name]\n","  f = lambda w: nn_forward_backward(params, toy_X, toy_y, reg=reg)[0]\n","  grad_numeric = eecs598.grad.compute_numeric_gradient(f, param)\n","  error = eecs598.grad.rel_error(grad, grad_numeric)\n","  print('%s max relative error: %e' % (param_name, error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LjAUalCBAOVd"},"source":["### Train the network\n","To train the network we will use stochastic gradient descent (SGD), similar to the SVM and Softmax classifiers. \n","\n","Look at the function `nn_train` and fill in the missing sections to implement the training procedure. This should be very similar to the training procedure you used for the SVM and Softmax classifiers. \n","\n","You will also have to implement `nn_predict`, as the training process periodically performs prediction to keep track of accuracy over time while the network trains. \n","\n","Once you have implemented the method, run the code below to train a two-layer network on toy data. Your final training loss should be less than 1.0."]},{"cell_type":"code","metadata":{"id":"Wgw06cLXAOVd"},"source":["import eecs598\n","from a2_helpers import get_toy_data\n","from two_layer_net import nn_forward_backward, nn_train, nn_predict\n","\n","eecs598.reset_seed(0)\n","toy_X, toy_y, params = get_toy_data()\n","\n","# YOUR_TURN: Implement the nn_train function.\n","#            You may need to check nn_predict function (the \"pred_func\") as well.\n","stats = nn_train(params, nn_forward_backward, nn_predict, toy_X, toy_y, toy_X, toy_y,\n","                 learning_rate=1e-1, reg=1e-6,\n","                 num_iters=200, verbose=False)\n","\n","print('Final training loss: ', stats['loss_history'][-1])\n","\n","# plot the loss history\n","plt.plot(stats['loss_history'], 'o')\n","plt.xlabel('Iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EUS4aDp_HzG1"},"source":["# Plot the loss function and train / validation accuracies\n","plt.plot(stats['train_acc_history'], 'o', label='train')\n","plt.plot(stats['val_acc_history'], 'o', label='val')\n","plt.title('Classification accuracy history')\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8cPIajWNAOVg"},"source":["## Testing our NN on a real dataset: CIFAR-10\n","Now that you have implemented a two-layer network that passes gradient checks and works on toy data, it's time to load up our favorite CIFAR-10 data so we can use it to train a classifier on a real dataset."]},{"cell_type":"code","metadata":{"id":"lYo_XrU3AOVg"},"source":["import eecs598\n","\n","# Invoke the above function to get our data.\n","eecs598.reset_seed(0)\n","data_dict = eecs598.data.preprocess_cifar10(dtype=torch.float64)\n","print('Train data shape: ', data_dict['X_train'].shape)\n","print('Train labels shape: ', data_dict['y_train'].shape)\n","print('Validation data shape: ', data_dict['X_val'].shape)\n","print('Validation labels shape: ', data_dict['y_val'].shape)\n","print('Test data shape: ', data_dict['X_test'].shape)\n","print('Test labels shape: ', data_dict['y_test'].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cq-HkgRBAOVQ"},"source":["### Wrap all function into a Class\n","We will use the class `TwoLayerNet` to represent instances of our network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are PyTorch tensors.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_CsYAv3uAOVi"},"source":["### Train a network\n","To train our network we will use SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate."]},{"cell_type":"code","metadata":{"id":"hgg0QV9DAOVj"},"source":["import eecs598\n","from two_layer_net import TwoLayerNet\n","\n","input_size = 3 * 32 * 32\n","hidden_size = 36\n","num_classes = 10\n","\n","# fix random seed before we generate a set of parameters\n","eecs598.reset_seed(0)\n","net = TwoLayerNet(input_size, hidden_size, num_classes, dtype=data_dict['X_train'].dtype, device=data_dict['X_train'].device)\n","\n","# Train the network\n","stats = net.train(data_dict['X_train'], data_dict['y_train'],\n","                  data_dict['X_val'], data_dict['y_val'],\n","                  num_iters=500, batch_size=1000,\n","                  learning_rate=1e-2, learning_rate_decay=0.95,\n","                  reg=0.25, verbose=True)\n","\n","# Predict on the validation set\n","y_val_pred = net.predict(data_dict['X_val'])\n","val_acc = 100.0 * (y_val_pred == data_dict['y_val']).double().mean().item()\n","print('Validation accuracy: %.2f%%' % val_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ixxgq5RKAOVl"},"source":["### Debug the training\n","With the default parameters we provided above, you should get a validation accuracy less than 10% on the validation set. This isn't very good.\n","\n","One strategy for getting insight into what's wrong is to plot the loss function and the accuracies on the training and validation sets during optimization.\n","\n","Another strategy is to visualize the weights that were learned in the first layer of the network. In most neural networks trained on visual data, the first layer weights typically show some visible structure when visualized."]},{"cell_type":"code","metadata":{"id":"6sYXImDTAOVm"},"source":["# Plot the loss function and train / validation accuracies\n","from a2_helpers import plot_stats\n","\n","plot_stats(stats)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"616EK5UoKgmE"},"source":["Similar to SVM and Softmax classifier, let's visualize the weights."]},{"cell_type":"code","metadata":{"id":"FnuRjtyKAOVo"},"source":["from a2_helpers import show_net_weights \n","\n","show_net_weights(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OlVbXxmPNzPY"},"source":["### What's wrong?\n","Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy."]},{"cell_type":"markdown","metadata":{"id":"rDNZ8ZAnN7hj"},"source":["#### Capacity?\n","Our initial model has very similar performance on the training and validation sets. This suggests that the model is underfitting, and that its performance might improve if we were to increase its capacity.\n","\n","One way we can increase the capacity of a neural network model is to increase the size of its hidden layer. Here we investigate the effect of increasing the size of the hidden layer. The performance (as measured by validation-set accuracy) should increase as the size of the hidden layer increases; however it may show diminishing returns for larger layer sizes."]},{"cell_type":"code","metadata":{"id":"_C-ChHUlN68f"},"source":["import eecs598\n","from a2_helpers import plot_acc_curves\n","from two_layer_net import TwoLayerNet\n","\n","hidden_sizes = [2, 8, 32, 128] \n","lr = 0.1\n","reg = 0.001\n","\n","stat_dict = {}\n","for hs in hidden_sizes:\n","  print('train with hidden size: {}'.format(hs))\n","  # fix random seed before we generate a set of parameters\n","  eecs598.reset_seed(0)\n","  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n","  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n","            num_iters=3000, batch_size=1000,\n","            learning_rate=lr, learning_rate_decay=0.95,\n","            reg=reg, verbose=False)\n","  stat_dict[hs] = stats\n","\n","plot_acc_curves(stat_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QpSrK3olUfOZ"},"source":["#### Regularization?\n","Another possible explanation for the small gap we saw between the train and validation accuracies of our model is regularization. In particular, if the regularization coefficient were too high then the model may be unable to fit the training data.\n","\n","We can investigate the phenomenon empirically by training a set of models with varying regularization strengths while fixing other hyperparameters.\n","\n","You should see that setting the regularization strength too high will harm the validation-set performance of the model:"]},{"cell_type":"code","metadata":{"id":"DRPsxxFnU3Un"},"source":["import eecs598\n","from a2_helpers import plot_acc_curves\n","from two_layer_net import TwoLayerNet\n","\n","hs = 128\n","lr = 1.0\n","regs = [0, 1e-5, 1e-3, 1e-1]\n","\n","stat_dict = {}\n","for reg in regs:\n","  print('train with regularization: {}'.format(reg))\n","  # fix random seed before we generate a set of parameters\n","  eecs598.reset_seed(0)\n","  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n","  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n","            num_iters=3000, batch_size=1000,\n","            learning_rate=lr, learning_rate_decay=0.95,\n","            reg=reg, verbose=False)\n","  stat_dict[reg] = stats\n","\n","plot_acc_curves(stat_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3zFWkxebWXtu"},"source":["#### Learning Rate?\n","Last but not least, we also want to see the effect of learning rate with respect to the performance."]},{"cell_type":"code","metadata":{"id":"lc_YYCDmWld-"},"source":["import eecs598\n","from a2_helpers import plot_acc_curves\n","from two_layer_net import TwoLayerNet\n","\n","hs = 128\n","lrs = [1e-4, 1e-2, 1e0, 1e2]\n","reg = 1e-4\n","\n","stat_dict = {}\n","for lr in lrs:\n","  print('train with learning rate: {}'.format(lr))\n","  # fix random seed before we generate a set of parameters\n","  eecs598.reset_seed(0)\n","  net = TwoLayerNet(3 * 32 * 32, hs, 10, device=data_dict['X_train'].device, dtype=data_dict['X_train'].dtype)\n","  stats = net.train(data_dict['X_train'], data_dict['y_train'], data_dict['X_val'], data_dict['y_val'],\n","            num_iters=3000, batch_size=1000,\n","            learning_rate=lr, learning_rate_decay=0.95,\n","            reg=reg, verbose=False)\n","  stat_dict[lr] = stats\n","\n","plot_acc_curves(stat_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVCEro4FAOVq"},"source":["### Tune your hyperparameters\n","\n","**Tuning**. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, number of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.\n","\n","**Plots**. To guide your hyperparameter search, you might consider making auxiliary plots of training and validation performance as above, or plotting the results arising from different hyperparameter combinations as we did in the Linear Classifier notebook. You should feel free to plot any auxiliary results you need in order to find a good network, but we don't require any particular plots from you.\n","\n","**Approximate results**. To get full credit for the assignment, you should achieve a classification accuracy above 50% on the validation set.\n","\n","(Our best model gets a validation-set accuracy 56.44% -- did you beat us?)"]},{"cell_type":"code","metadata":{"id":"bG4DjBMIAOVq"},"source":["import os\n","import eecs598\n","from two_layer_net import TwoLayerNet, find_best_net, nn_get_search_params\n","\n","# running this model on float64 may needs more time, so set it as float32\n","eecs598.reset_seed(0)\n","data_dict = eecs598.data.preprocess_cifar10(dtype=torch.float32)\n","\n","# store the best model into this \n","eecs598.reset_seed(0)\n","best_net, best_stat, best_val_acc = find_best_net(data_dict, nn_get_search_params)\n","print(best_val_acc)\n","\n","plot_stats(best_stat)\n","\n","# save the best model\n","path = os.path.join(GOOGLE_DRIVE_PATH, 'nn_best_model.pt')\n","best_net.save(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NsYIu49plJ9r"},"source":["# Check the validation-set accuracy of your best model\n","y_val_preds = best_net.predict(data_dict['X_val'])\n","val_acc = 100 * (y_val_preds == data_dict['y_val']).double().mean().item()\n","print('Best val-set accuracy: %.2f%%' % val_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZgDq4zlAOVt"},"source":["from a2_helpers import show_net_weights\n","# visualize the weights of the best network\n","show_net_weights(best_net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UG56gKWsAOVv"},"source":["### Run on the test set\n","When you are done experimenting, you should evaluate your final trained network on the test set. To get full credit for the assignment, you should achieve over 50% classification accuracy on the test set.\n","\n","(Our best model gets 56.03% test-set accuracy -- did you beat us?)"]},{"cell_type":"code","metadata":{"id":"2b3h8f8_AOVw"},"source":["y_test_preds = best_net.predict(data_dict['X_test'])\n","test_acc = 100 * (y_test_preds == data_dict['y_test']).double().mean().item()\n","print('Test accuracy: %.2f%%' % test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"37R_J2uMP3d-"},"source":["# Submit Your Work\n","After completing both notebooks for this assignment (`linear_classifier.ipynb` and this notebook, `two_layer_net.ipynb`), run the following cell to create a `.zip` file for you to download and turn in. **Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"]},{"cell_type":"code","metadata":{"id":"GE2M6t1hP-Pe"},"source":["from eecs598.submit import make_a2_submission\n","\n","# TODO: Replace these with your actual uniquename and umid\n","uniquename = None\n","umid = None\n","make_a2_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGbRHnoEAUVN"},"source":[""],"execution_count":null,"outputs":[]}]}